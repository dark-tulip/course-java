## Kafka
- distributed streaming system
- horizontal scaling via brokers
- позволяет уменьшить количество интеграций
- успешна на real time переносах данных

## Настройки по умочанию - первым делом когда разворачиваете кластер
- `default.replication.factor=1`
- `auto.create.topics.enable=true` (когда пишут в несуществующий топик, применяются настройки по умолчанию)

### Размер на уровне топика и брокера
- `log.retention.bytes (на уровне брокера, unlimited)
- `retention.bytes` (на уровне топика, PER PARTITION)
- партиция это некоторая единица параллелизации

## Kafka Topics
- непрерывный поток данных
- как таблица в БД, но без constraint-ов, и тут нельзя делать запросы
- у каждого топика есть имя, по которому топик идентифицируется
- топик кафки `IMMUTABLE` - нельзя изменить или удалить данные с партиции, можно только дозаписывать
- чтобы записать данные в топик - нужно знать адрес любого брокера и название топика
- to read data from topic you need any broker and topic name

## Partitions and offsets
- каждый топик разделен на **партиции**, партиция которая содрержит message-ы в упорядоченном формате
- каждое сообщение содержит инкрементальный id в партиции, который называется **Kafka partition offset**-ом
- данные хранятся ограниченное время (default 1 week)
- offset-ы относятся ТОЛЬКО к ПАРТИЦИЯМ, у каждой партиции свой оффсет
- оффсеты НЕ переиспользуются, даже если данные были удалены
- порядок оффсета обеспечивается в каждой партиции отдельно, данные между партициями НЕ пересекаются
- **LEADER** and **ISR**

# Kafka partitions
- партиции хранятся на диске;
- партиции разделены на сегменты;
- партиции как единицы параллелизации хранения данных в топике;
- данные можно удалять только целыми сегментами;
- данные удаляются целыми сегментами либо по времени, либо по размеру;
- `retention.bytes=-1` (размер партиции ограничем размером диска)
- `retention.ms=604800000` - 1 неделя (данные хранятся в партиции)
- для каждого сегмента на диске хранится три файла `segment{basic_offset, data, index, timeindex}`
```
basic_offset is 0000123456789 (segment name)
data is         0000123456789.log (append only подход записи)
index is        0000123456789.index (чтобы быстро искать по offset)
timeindex is    0000123456789.timeindex (у каждого события есть время)
```
- сообщение можно быстро найти по его offset-у

## topic replication factor
- ONLY ONE BROKER can be leader of partition
- producers send data only for leader broker
- все реплики называются ISR - **IN SYNC REPLICA**
- у каждой партиции может быть только один ЛИДЕР, в которую записыввают и по умолчанию читают данные
- реплики ТОЛЬКО РАДИ репоикации данных (since v2.4+ can read from closest ISR - why? - network cost and latency)

## Producer Acknowledgements (acks) and durability
- acks=0 - won't wait acknowledgement (possible data loss)
- acks=1 - leader acknowledgement (limited data loss)
- acks=all - leader + replicas acknowledgement (no data loss)
- имея фактор репликации в N, имея выбивших из строя N-1 брокера, можно восстановить данные

## Zookeeper - первое хранилище метаданных (leaders, partitions)
- используется для упраления брокерами кафки, точнее хранит их список
- zookeeper used to manage kafka brokers
- helps to perform leader elections
- отправляет уведомления кафке об изменениях в конфигурации
- зукипер работает с нечетным числом серверов (1, 3, 5, 7)
- kafka 2.* cannot work without zookeeper (goes together)
- зукипер существует с момента появления кафки
- kafka 3.* may have kafka Raft instead of zookeeper (KIP-500)
- до кафки 4.0 желательно всегда использовать зукипер
- ЗУКИПЕР НИКОГДА НЕ хранит consumer offsets (ложная инфа в интернете)
- zeekeeper leader server (write), others, followers (for read)

### Зачем и когда нужен зукипер?
- когда работают внутренне с брокерами кафки
- не нужен клиентам кафки (по соображениям безопасности)
- весь API и CLI для работы с клиентами кафки уже мигрировал с зукипера в брокеры кафки
- порты зукпера должны быть открыты только кафка брокерам, НЕ кафка клиентам
- НИКОГДА не используйте зукипер в качестве конфигов для кафка клиентов, и других программ, которые connected с кафкой

### KRAFT - Raft протокол пришедший на замену зукиперу
- зукипер страдает по производительности когда кластер кафки имеет более 100_000 партиций
- единая модель безопасности
- слияние с кафкой изнутри
- быстрее время отклика, recovery и shutdown time (planned and unplanned)
- production ready since 3.3.1+
- kafka 4.+ больше не поддерживает зукипер
- проект KIP-500 проект по удалению зукипера из кафки
- to deploy more partitions per cluster

Выборы лидера zK основаны на протоколе, похожем на Paxos, который называется ZAB . Каждая запись проходит через лидера, а лидер генерирует идентификатор транзакции (zxid) и присваивает его каждому запросу на запись. Идентификатор представляет порядок, в котором записи применяются ко всем репликам. Запись считается успешной, если лидер получает подтверждение от большинства. Пояснение к ЗАБ .

## Quorum controller
- event driver architecture
- run in kafka cluster itself
- хранит свое событийное состояние так, что событийная модель  может быть воссоздана
- **events log stores this state**
- другие контроллеры читают события leader quorum controller-a, и сохраняют в своем журнале
- так пропущенные события могут легко восстановиться по журанлу событий
- **metadata топик** is events store
